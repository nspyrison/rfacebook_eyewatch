---
title: "Eyewatch facebook posts -- via Hootsuite"
author: "Nicholas Spyrison"
date: "July 2018"
output: 
  html_document:
    self_contained: true
---
```{r options, include=FALSE}
knitr::opts_chunk$set(
  echo=TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 8,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE)

library("readr")
library("dplyr")
library("tibble")
library("tsibble")
library("lubridate")
library("plotly")
#library(tidyverse)
```


```{r}
### New report: "Facebook_PostReactions_ns" 
# Can schedule exporting weekly and monthly, but on only to hootsuite emaills.
# Data in hootsuite back to 18 Nov 2017.
# Nick has access to all 55 stations as of 20/09/2018.

### 12 CSN stations:
## Ballarat, Brimbank, Cardinia, Frankston, Geelong, Greater Dandenong,
## Greater Shepparton, Knox, Latrobe, Melton, Whittlesea, Wyndham.

# posts between: 18-11-2017 to 19-09-2018 for the 12 CSN stations
# Hootsuite doesn't have data before 18 Nov 2017. Also have all 55 station file.

readr::read_csv("./data/facebook_postreactions_ns_2017-11-18_to_2018-09-19_created_on_20180920T0920Z_facebook_posts.csv"
) %>% as_tibble() -> dat_in

#View(dat_in)
#str(dat_in)
dim(dat_in)
```

A raw data set of 5,674 facebook posts with 9 variables of the 12 CSN Eyewatch Stations was exported from Hootsuite on 20 September 2018. The data ranges from 18 November 2017 through 19 September 2018. Post before 18 November 2017 are not available on the Hootsuite platform.

After cleaning and tidying the data, the 5,674 posts had 27 variables. The increase is for ease of reporting and filtering. The following graphs were produced from this data.

```{r, eval=F,include=F}
dat <- dat_in[, c(1:6, 8:10)] # remove empty 'Tags' and 'Ow.ly Clicks' columns

dat <- rename(.data = dat, 
       `Datetime[GMT]` = `Date (GMT)`,
       `Station` = `Facebook Page`,
       `PagePostID`    = `Post ID`,
       `PostLink`      = `Post Permalink`,
       `PostType`      = `Post Type`,
       `PostMessage`   = `Post Message`,
       `ReactionCount` = `Reactions`,
       `CommentCount`  = `Comments`,
       `ShareCount`    = `Shares`
)

dat <- mutate(
  .data = dat,
  `Station`            = as.factor(substr(`Station`, 12, 100)),
  `Date[GMT]`          = as.Date(`Datetime[GMT]`),
  `Time[GMT]`          = substr(`Datetime[GMT]`, 12, 23),
  `Hour[GMT]`          = as.factor(substr(`Datetime[GMT]`, 12, 13)),
  `Year`               = year(`Datetime[GMT]`),
  `Quarter`            = quarter(`Datetime[GMT]`, with_year = F),
  `Month`              = month(`Datetime[GMT]`, abbr = T),
  `Week`               = week(`Datetime[GMT]`),
  `YearQuarter`        = tsibble::yearquarter(`Datetime[GMT]`),
  `YearMonth`          = tsibble::yearmonth(`Datetime[GMT]`),
  `YearWeek`           = tsibble::yearweek(`Datetime[GMT]`),
  `Weekday`            = wday(`Datetime[GMT]`, label = TRUE, week_start = 1),
  `PostDaysOld`        = as.integer(max(`Date[GMT]`) - `Date[GMT]`),
  `PostType`           = as.factor(`PostType`),
  `ReactionCount`      = as.integer(`ReactionCount`),
  `CommentCount`       = as.integer(`CommentCount`),
  `ShareCount`         = as.integer(`ShareCount`),
  `EngagementCount`    = as.integer(`ReactionCount` + `CommentCount` +
                                      `ShareCount`),
  `HasVicpolLink`      = (grepl("vicpol", `PostLink`) & `PostType` == "link"),
  `HasHttpsVicpolLink` = (grepl("https", `PostLink`) & `HasVicpolLink` == T)
)

clean_dat <- dat[,
  c("PagePostID", "Station",  "Datetime[GMT]", "PostLink", "PostType", 
    "PostMessage", "ReactionCount", "CommentCount", "ShareCount", 
    "EngagementCount", "HasVicpolLink", "HasHttpsVicpolLink", "Date[GMT]", 
    "Time[GMT]", "Hour[GMT]", "Year", "Quarter", "Month", "Week", "YearQuarter",
    "YearMonth", "YearWeek", "Weekday", "PostDaysOld")
  ]

#save(clean_dat, file="./data/last_clean_dat.rda")
```

## Posts by time

```{r, echo=F}
#load(file="./data/last_clean_dat.rda")
filt_dat <- clean_dat

nrow(filt_dat) # originally 5674
filt_dat <- dplyr::filter(filt_dat, HasVicpolLink == F)
nrow(filt_dat) # still 5674
filt_dat <- dplyr::filter(filt_dat, PostType != "music")
nrow(filt_dat) # still 5674

(p1 <-
    ggplot(data = filt_dat, aes(x = `Date[GMT]`) ) +
    geom_histogram(bins = length(unique(filt_dat$YearMonth) ) ) +
    ylab(label = "Post Count") + scale_fill_brewer(palette = "Dark2") )

sub <- sub[c("YearMonth", "ReactionCount", "CommentCount", "shareCount")]
agg <- summarize(sub,
                 ReactionMean   = mean(ReactionCount),
                 CommentMean    = mean(CommentCount),
                 SharesCount    = mean(SharesCount),
                 PostMean       = n()/length(unique(YearMonth)),
                 YearMonthCount = length(unique(YearMonth)),
                 PostCount      = n()
                 )
agg
```
There is high variability in number of posts per month. On average 24.3 posts are made per month exluding the first 4 months. Each post, on average recieves 26.3 likes, 4.9 comments and 21.4 shares.


## Mean shares by time across type

```{r, include=F}
sub <- filt_dat[c("YearMonth", "PostType", "ReactionCount",
                  "CommentCount", "ShareCount")]
gb_sub <- group_by(sub, YearMonth, PostType)
gb_agg <- summarize(gb_sub, 
                 ReactionMean   = mean(ReactionCount),
                 CommentMean    = mean(CommentCount),
                 Shares_mean    = mean(ShareCount),
                 ReactionSum    = sum(ReactionCount),
                 CommentSum     = sum(CommentCount),
                 ShareSum       = sum(ShareCount),
                 YearMonthCount = n()
                 )

(p1.2 <-
    ggplot(filt_dat ,aes(x = Date[GMT], y = ShareCount, fill = PostType ) ) +
    geom_smooth() + ylab(label = "Mean shares per post") +
    scale_fill_brewer(palette = "Dark2") )
```

Link posts tend to recieve the most shares per post, status posts use to receive more shares per post than the other types, this may be a an artifact only having a few.

## Posts by hour across weekday

```{r, echo=F}
(p2 <- 
  ggplot(bal_posts_clean, aes(x = created_hour, fill = created_weekday) ) + 
  geom_histogram(stat="count") + 
  ylab(label = "posts_count") + scale_fill_brewer(palette = "Dark2")
)  
```

Posts are much more common on weekdays than weekends as expected. Posts are not lowest around early morning hours, suggesting a timezone offset. It looks like there might be a higher proportion of posts on hours "01" and "04", suggesting a systamatic process possibly calendar event ortimed release of news.


## Comments and likes by post age

```{r, echo=F}
p3 <- 
  ggplot(data = bal_posts_clean, 
         aes(x = likes_count, y = comments_count, 
             shares_count = shares_count, type = type, id = id, 
             color = -post_age)) + 
  geom_point(alpha=.5) + scale_fill_brewer(palette = "Dark2")
plotly::ggplotly(p3)
```

Newer posts appear to receive more comments than older posts. Older posts may or may not receive for likes (hard to tell at this color and transparency)


## Shares and likes by type

```{r, echo=F}
p4 <- 
  ggplot(data = bal_posts_clean, 
         aes(x = likes_count, y = shares_count, 
             shares_count = shares_count, id = id, 
             color = type) ) +
  geom_point(alpha=.5) + scale_fill_brewer(palette = "Dark2")
plotly::ggplotly(p4)
```

Extreme values tend to be photos. Extreme values tend to be liked or shared, but not both. Note that photos are the most common posts followed by links.


## Like by year-month across type of post

```{r, echo=F}
bal_posts_clean$type <- 
  factor(bal_posts_clean$type, levels = 
           rev(c("photo", "link", "video", "status", "music") ) )
(p5 <-
    ggplot(data = bal_posts_clean, 
       aes(x = created_yearMonth, y = likes_count, fill = type)) +
    geom_bar(stat = 'identity') + scale_fill_brewer(palette = "Dark2") 
)
```

Photos followed by links generate the most likes. Note that the y axis is likes and not likes/post.


### Missingness and pair-wise scatterplot

```{r, echo=F}
(p0 <- visdat::vis_dat(bal_posts_clean))
```

The data is mostly present. `story, message, link` are the variables by descending missingness.

```{r, echo=F}
sub <- 
  bal_posts_clean[c("created_date", "created_weekday", 
                    "likes_count", "comments_count", "shares_count", "type")]
(p00 <- GGally::ggpairs(data = sub))
```



### Metadata

page: eyewatchballarat
pull time: 1.04 min
raw dim: 952 obs x 11 var 
clean dim: 95 2obs x 26 var 
pull time: 5:30pm 16 May 2018
```{r, eval=F, include=F}
library(Rfacebook)

token <- "EAACEdEose0cBAPTWv9SPehGkMUhTbWCcBYkTi2FNvhAlETSACg0ZCrTxK94v7QSLXWJQxniD8aVkHob0541svS4WOVpKv8O1PABqXXtQWGusAMPdLZBYYRjo6MlOGVUHQQQCOEaoPZAoRWDXbKT8d7lBSId2mBvxOmqxUHJIbKE8UIObWPDOYDaXta9oxAZD" 
# temp token, will need to gen each time, see:
# https://developers.facebook.com/tools/explorer/?method=GET&path=me%3Ffields%3Did%2Cname&version=v3.0)

bal <- "eyewatchballarat"

start_time <- Sys.time()
bal_posts_raw <- as_tibble(getPage(bal, token, n=1000, feed = TRUE)) 
end_time <- Sys.time()

end_time - start_time #1.04 min, 952 x 11. ballarat, 5:30pm 16 May 2018
save(bal_posts_raw, file = "data/bal_posts_raw.rda")
```
