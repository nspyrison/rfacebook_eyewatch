---
title: "Eyewatch facebook posts -- via Hootsuite"
author: "Nicholas Spyrison"
date: "July 2018"
output: 
  html_document:
    self_contained: true
---
```{r options, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "",
  fig.height = 8,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE)

library("readr")
library("dplyr")
library("tibble")
library("tsibble")
library("lubridate")
library("plotly")
```


```{r, include=F}
### New report: "Facebook_PostReactions_ns" 
# Can schedule exporting weekly and monthly, but on only to hootsuite emaills.
# Data in hootsuite back to 18 Nov 2017.
# Nick has access to all 55 stations as of 20/09/2018.

### 12 CSN stations:
## Ballarat, Brimbank, Cardinia, Frankston, Geelong, Greater Dandenong,
## Greater Shepparton, Knox, Latrobe, Melton, Whittlesea, Wyndham.

# posts between: 18-11-2017 to 19-09-2018 for the 12 CSN stations
# Hootsuite doesn't have data before 18 Nov 2017. Also have all 55 station file.

# All times reported in GMT.

filename <- "../data/facebook_postreactions_ns_2017-11-18_to_2018-09-19_created_on_20180920T0920Z_facebook_posts.csv"
dat_in <- readr::read_csv(filename) %>% as_tibble() 

dat <- dat_in[, c(1:6, 8:10)] # remove empty columns: 'Tags' and 'Ow.ly Clicks'
dat <- rename(.data = dat, 
              `Datetime`      = `Date (GMT)`,
              `Station`       = `Facebook Page`,
              `PagePostID`    = `Post ID`,
              `PostLink`      = `Post Permalink`,
              `PostType`      = `Post Type`,
              `PostMessage`   = `Post Message`,
              `ReactionCount` = `Reactions`,
              `CommentCount`  = `Comments`,
              `ShareCount`    = `Shares`
)

dat <- mutate(
  .data = dat,
  `Station`            = as.factor(substr(`Station`, 12, 100)),
  `Date`          = as.Date(`Datetime`),
  `Time`          = substr(`Datetime`, 12, 23),
  `Hour`          = as.factor(substr(`Datetime`, 12, 13)),
  `Year`               = year(`Datetime`),
  `Quarter`            = quarter(`Datetime`, with_year = F),
  `Month`              = month(`Datetime`, abbr = T),
  `Week`               = week(`Datetime`),
  `YearQuarter`        = tsibble::yearquarter(`Datetime`),
  `YearMonth`          = tsibble::yearmonth(`Datetime`),
  `YearWeek`           = tsibble::yearweek(`Datetime`),
  `Weekday`            = wday(`Datetime`, label = TRUE, week_start = 1),
  `PostDaysOld`        = as.integer(max(`Date`) - `Date`),
  `PostType`           = as.factor(`PostType`),
  `ReactionCount`      = as.integer(`ReactionCount`),
  `CommentCount`       = as.integer(`CommentCount`),
  `ShareCount`         = as.integer(`ShareCount`),
  `EngagementCount`    = `ReactionCount` + `CommentCount` + `ShareCount`,
  `PostMessage`        = 
    ifelse(`PostMessage` == "(Post with no description)", NA, `PostMessage`),
  `HasVicpolLink`      = (grepl("vicpol", `PostLink`) & `PostType` == "link"),
  `HasHttpsVicpolLink` = (grepl("https", `PostLink`) & `HasVicpolLink` == T)
)

clean_dat <- dat[,
  c("PagePostID", "Station",  "Datetime", "PostLink", "PostType", 
    "PostMessage", "ReactionCount", "CommentCount", "ShareCount", 
    "EngagementCount", "HasVicpolLink", "HasHttpsVicpolLink", "Date", 
    "Time", "Hour", "Year", "Quarter", "Month", "Week", "YearQuarter",
    "YearMonth", "YearWeek", "Weekday", "PostDaysOld")
  ]
```

A raw data set of `r nrow(dat)` facebook posts with `r ncol(dat)` variables of `r nrow(unique(clean_dat[, 2]))` unique stations was exported from Hootsuite. The data ranges from `r min(clean_dat$Date)` through `r max(clean_dat$Date)`. Posts before 18 November 2017 are not available on the Hootsuite platform.

```{r, eval=T,include=F}
#save(clean_dat, file="../data/last_clean_dat.rda")
#load(file="../data/last_clean_dat.rda")

filt_dat <- clean_dat

filt_dat <- dplyr::filter(filt_dat, HasVicpolLink == F)
filt_dat <- dplyr::filter(filt_dat, PostType != "music")


startDate <- min(filt_dat$`Date`)
endDate   <- max(filt_dat$`Date`)
startDay  <- as.integer(substr(startDate,9,10))
endDay    <- as.integer(substr(endDate,9,10))
startDaysAfterFirst <- startDay - 1
endDaysBeforeLast <- 
  as.integer(lubridate::days_in_month(max(filt_dat$`Date`)) - endDay)
if (startDaysAfterFirst > 3) {
  daysTillEoM <- as.integer(
    lubridate::days_in_month(min(filt_dat$`Date`)) - startDay)
  dateFloor <- lubridate::as_date(min(filt_dat$`Date`) + daysTillEoM)
  filt_dat <- dplyr::filter(filt_dat, `Date` > dateFloor)
}
if (endDaysBeforeLast > 3) {
  dateCeiling <- lubridate::as_date(max(filt_dat$`Date`) - endDay + 1)
  filt_dat <- dplyr::filter(filt_dat, `Date` < dateCeiling)
}

### Order PostType by asc mean EngagementCount
# levels(filt_dat$PostType)
filt_dat$PostType = factor(
  filt_dat$PostType, levels = c("Event", "Video", "Link", "Status", "Photo"))

sub <- filt_dat[c("Station", "YearMonth", "ReactionCount", "CommentCount",
                  "ShareCount", "EngagementCount")]
agg <- summarize(.data = sub,
                 CountStaions     = length(unique(Station)),
                 CountYearMonths  = length(unique(YearMonth)),
                 MeanPostsPm      = n()/length(unique(YearMonth)),
                 MeanReactionsPp  = mean(ReactionCount),
                 MeanSharesPp     = mean(ShareCount),
                 MeanCommentsPp   = mean(CommentCount),
                 MeanEngagementPp = mean(EngagementCount)
)
```
After cleaning and tidying the data; we filter out link posts that contain "vicpol" in the post url and filter out posts in. If the first month and last month aren't a full months worth of data we remove them for a more apples to apples comparison. The following graphs were produced from the remaining `r nrow(clean_dat)` posts (`r nrow(dat_in) - nrow(clean_dat)` posts less than the original set).

## Posts by time

```{r, echo=F}
(g1 <-
  ggplot(data = filt_dat, aes(x = `Date`) ) +
  geom_histogram(bins = length(unique(filt_dat$YearWeek) ) ) +
  ylab(label = "Post Count") + xlab("Time (bar is 1 week)") +
  scale_fill_brewer(palette = "Dark2") )
```
Across the `r agg$CountStations` stations for the remaining `r agg$CountYearMonths` months, on average there are: `r agg$MeanPostsPm` posts per month, `r agg$MeanReactionsPp` reactions per post, `r agg$MeanSharesPp` shares per post, `r agg$MeanCommentsPp` comments per post, and `r agg$MeanEngagementsPp` engagements per post. We define engagement to be the sum of reactions, shares, and comments.

## Engagement by month across post type

```{r, echo=F}
(g4 <- ggplot(data = filt_dat, 
              aes(x = YearMonth, y = EngagementCount, fill = PostType) ) +
   geom_bar(stat = 'identity') + 
   scale_fill_brewer(palette = "Dark2") + xlab("Time (bar is 1 month)")  )
```

The bulk of engagements are with photo posts.

## Mean engagements trends by time across post type

Do we see engagement across post type changing over time?

```{r, include=F}
(g2 <- ggplot(filt_dat, 
              aes(x = `Date`, y = EngagementCount, fill = PostType ) ) +
   geom_smooth() + scale_fill_brewer(palette = "Dark2") + xlab("Time") + 
   coord_cartesian(ylim = c(0, 100)) 
)
```

## Engagement by post type across post age

Of the bottom 95% of posts by engagement.

```{r, echo=F}
p3 <- ggplot(data = filt_dat, 
             aes(x = PostType, y = EngagementCount, 
                 color = -PostDaysOld, id = PagePostID)) + 
  geom_point(alpha = .4, position = "jitter") + 
  scale_y_continuous(limits = c(0, quantile(filt_dat$EngagementCount, .95)) ) 
plotly::ggplotly(p3)
```

## High engagment posts: engagement by post type across post age 

Taking a closer look at high engagement posts. Looking at the same graph for only the top 10% of engaged posts.

```{r, echo=F}
p4 <- ggplot(data = filt_dat, 
             aes(x = PostType, y = EngagementCount, 
                 color = -PostDaysOld, id = PagePostID)) + 
  geom_point(alpha = .6, position = "jitter") + 
  scale_y_continuous(limits = 
                       c(quantile(filt_dat$EngagementCount, .90),
                         max(filt_dat$EngagementCount)*1.01 ) 
  ) + scale_fill_brewer(palette = "Dark2")
plotly::ggplotly(p4)
```

### Appendix

All times are listed in GMT.

```{r, echo=F}
(a1 <- visdat::vis_dat(filt_dat))
```

The data is mostly present. `story` is NA if there is no message with a post.

```{r, echo=F}
a2sub <- filt_dat[c("Station", "YearMonth", "ReactionCount", "CommentCount",
                  "ShareCount", "EngagementCount")]
(a2 <- GGally::ggpairs(data = a2sub))
```


```{r, echo=F}
a3sub <- 
  filt_dat[c("Station", "Date","Weekday", "PostDaysOld", "PostType",
             "ReactionCount", "CommentCount", "ShareCount", "EngagementCount")]
summary(a3sub)
```